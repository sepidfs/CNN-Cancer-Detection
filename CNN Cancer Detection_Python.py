# -*- coding: utf-8 -*-
"""Colab_Histo_DataLoader_and_EDA_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B8wX-Ta4eONwzxjVz-DqE0skDFyvfnp4

# Histopathologic Cancer Detection — CNN Classification
### Sepideh Forouzi

## 1. Introduction

Metastatic cancer diagnosis through histopathology slides is a critical step in clinical oncology.  
This project participates in the **Kaggle [Histopathologic Cancer Detection](https://www.kaggle.com/c/histopathologic-cancer-detection/overview)** competition, where the task is to identify cancerous tissue in microscopic biopsy patches.

The dataset consists of **96×96 RGB images**, each extracted from high-resolution digital pathology scans.  
Each image is labeled as:

- **1** → metastatic cancer is present  
- **0** → no cancer detected  

The training set includes **220,025** image patches and the test set contains **57,468** unlabeled patches.  
These are small tiles, but the textures, nuclei patterns, and staining differences hold microscopic signals that deep learning can exploit.

---

### Project Goal

The goal of this work is to:
- Build deep learning models capable of detecting cancer in histopathologic images.  
- Compare multiple CNN architectures and evaluate their performance.  
- Handle class imbalance, regularization, and training stability challenges.  
- Generate a `submission.csv` file for the Kaggle leaderboard and attain a competitive score.

---

### Methods Used

In this notebook, we:
-  Perform data loading, inspection, and exploratory data   analysis  
-  Visualize sample images and class distribution  
-  Build and compare several CNN-based architectures, including:
  - **Baseline CNN**  
  - **Deep CNN**  
  - **Double Convolution CNN**  
  - **Double Deep Architecture**

### Apply regularization techniques (BatchNormalization, Dropout, L2)  
-  Use class weighting to address dataset imbalance  
-  Evaluate using multiple metrics: **AUC, Accuracy, Precision, Recall, Loss**  
- Export a `submission.csv` file and upload to the competition platform for scoring.

---

### Why Convolutional Neural Networks?

The image patches contain spatial-texture cues such as nuclei density, texture irregularities, and staining variation.  
Convolutional neural networks (CNNs) are highly suitable for this domain, because they:
- Learn hierarchical spatial features from small patches  
- Leverage translation invariance across local textures  
- Perform efficiently even for moderate-sized input (96×96) when properly regularized

---

### Achievements & Strengths

- Comprehensive comparative study of several architectures rather than a single model  
- Transparent methodology: all hyperparameters, model definitions, and metrics are documented  
- Reproducible pipeline: data split, class weights, augmentations, training logs  
- Submission to a recognized, high-visibility Kaggle competition (link provided above)

---

This work focuses not only on achieving a strong score but also on methodological rigor, interpretability, and reproducibility.

---
### Step 1 -Data Access, Integrity Checks, and Basic Summary

**What I did.**  
I mounted Google Drive, extracted the competition archive into `/content/histopathologic-cancer-detection/`, and verified the expected paths:
- `DATA_DIR`, `TRAIN_DIR`, `TEST_DIR`, and `train_labels.csv` are all present (`True`).
- I then loaded `train_labels.csv` into a DataFrame (`labels`) and created a working copy `df`.

**Sanity checks.**  
- Shape: `df` contains **220,025** rows and 2 columns (`id`, `label`).
- Dtypes: `id` is `object` (string), `label` is `int64` (binary 0/1).
- Missingness: **0** missing values in both columns.
- Duplicates: **0** duplicated rows; **0** duplicated `id`s (each tile id is unique).

**Quick descriptive stats.**  
- `label` mean = **0.4050**, indicating class imbalance (~40.5% positives vs 59.5% negatives).
- `label` min/max = **0/1** (as expected for binary labels).

**Why this matters.**  
- Verified file system layout prevents downstream I/O errors.
- Uniqueness of `id` guarantees a one-to-one mapping to image files.
- No missing data means no imputation/row-dropping is required for labels.
- The positive rate (~40.5%) suggests using **class weighting** and careful **threshold analysis** during model evaluation.
"""

from google.colab import drive
drive.mount('/content/drive')

ZIP_PATH = "/content/drive/MyDrive/Colab Notebooks/histopathologic-cancer-detection.zip"
TARGET = "/content/histopathologic-cancer-detection"

!mkdir -p "$TARGET"
!unzip -q -o "$ZIP_PATH" -d "$TARGET"

print(" Extracted to:", TARGET)

from pathlib import Path
import pandas as pd

DATA_DIR   = Path("/content/histopathologic-cancer-detection")
TRAIN_DIR  = DATA_DIR / "train"
TEST_DIR   = DATA_DIR / "test"
LABELS_CSV = DATA_DIR / "train_labels.csv"

print("DATA_DIR exists:", DATA_DIR.is_dir())
print("TRAIN_DIR exists:", TRAIN_DIR.is_dir())
print("TEST_DIR exists:",  TEST_DIR.is_dir())
print("LABELS_CSV exists:", LABELS_CSV.is_file())

labels = pd.read_csv(LABELS_CSV)
print("labels shape:", labels.shape)
labels.head()

import pandas as pd
from IPython.display import display

df = labels.copy()

print(" Dataset loaded successfully!")
print(f"Total rows: {len(df):,}")
print("Columns:", df.columns.tolist())

print("\n First 5 rows:")
display(df.head())

print("\n Data types & memory:")
df.info()

print("\n Statistical summary:")
display(df.describe(include='all'))

print("\n Missing values per column:")
print(df.isnull().sum())

print("\n Duplicate rows:", df.duplicated().sum())
print(" Duplicate ids  :", df['id'].duplicated().sum())

"""### Step 2 - Label Integrity and Class Distribution

To ensure there are no label–image mismatches, I cross-checked every `id` in `train_labels.csv` with the `.tif` files actually present in `/train`:

- **220,025** labels in `train_labels.csv`
- **220,025** `.tif` tiles exist in the `train/` directory
- **0** labeled IDs are missing on disk
- **0** orphan `.tif` files exist without a label  
*All labeled samples have a corresponding image tile, and the dataset is internally consistent.*

Next, I inspected the class distribution:

- **0 → No metastasis**: 130,908 samples (≈ **59.5%**)
- **1 → Metastasis detected**: 89,117 samples (≈ **40.5%**)

This confirms a **moderate class imbalance**, where negative cases appear more frequently than positive cases.  
Such imbalance can bias a model toward predicting class `0`, so later I apply:

✔ class weighting during training  
✔ AUC, precision, and recall as evaluation metrics (not just accuracy)

The bar chart below visualizes the class imbalance:

"""

from pathlib import Path

# set of ids that actually exist in /train as .tif files
train_ids_disk = {p.stem for p in TRAIN_DIR.glob("*.tif")}
label_ids      = set(df['id'].tolist())

missing_on_disk  = label_ids - train_ids_disk
orphan_on_disk   = train_ids_disk - label_ids

print(f" Labeled IDs: {len(label_ids):,}")
print(f" Train .tif on disk: {len(train_ids_disk):,}")
print(f"  Labeled but missing .tif: {len(missing_on_disk):,}")
print(f"ℹ  Present on disk but not labeled (expected for test/train mix): {len(orphan_on_disk):,}")

# (Optional) peek at a few missing ids if any
list(sorted(missing_on_disk))[:5]

import matplotlib.pyplot as plt

class_counts = df['label'].value_counts().sort_index()
class_pct    = (df['label'].value_counts(normalize=True).sort_index() * 100).round(2)

print("Class counts:", class_counts.to_dict())
print("Class %:", class_pct.to_dict())

ax = class_counts.plot(kind="bar", rot=0)
ax.set_title("Class Balance (0 = No Metastasis, 1 = Metastasis)")
ax.set_xlabel("Label")
ax.set_ylabel("Count")
plt.show()

"""### Step 3 - Visual Inspection of Sample Tiles

As part of the exploratory data analysis, I randomly sampled images from each class and visualized them to better understand tissue morphology and staining variability.  
Each tile is a **96×96** histopathology patch extracted from large whole-slide images.

- **Label 0 – Non-metastatic (healthy) tissue**  
  Typically shows well-structured nucleus spacing and more organized tissue morphology.  
- **Label 1 – Metastatic (cancer) tissue**  
  Often appears with dense nuclei, irregular shapes, and disrupted tissue structure.

The visual inspection highlights:
- High variability in stain saturation, brightness, and background artifacts  
- Presence of tiles with very little tissue, which may introduce noise  
- Clear textural differences between healthy and cancerous regions  

This variability supports the use of convolutional neural networks, which are well-suited to learning localized texture features.  
The plotted samples below provide representative examples from each class.

"""

import numpy as np, cv2, itertools, math
import matplotlib.pyplot as plt

SEED = 2025
rng  = np.random.default_rng(SEED)

def sample_ids(df, lbl, k=12):
    k = min(k, int((df.label == lbl).sum()))
    return df.loc[df.label==lbl, 'id'].sample(k, random_state=SEED).tolist()

def plot_tiles(ids, root=TRAIN_DIR, ncols=6, title=None):
    if len(ids)==0:
        print("No images to show.")
        return
    nrows = math.ceil(len(ids)/ncols)
    fig, axes = plt.subplots(nrows, ncols, figsize=(2*ncols, 2*nrows))
    axes = np.array(axes).reshape(-1)
    for ax, img_id in itertools.zip_longest(axes, ids, fillvalue=None):
        ax.axis("off")
        if img_id is None:
            continue
        path = str(root / f"{img_id}.tif")
        im = cv2.imread(path, cv2.IMREAD_COLOR)
        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
        ax.imshow(im); ax.set_title(img_id[:10])
    if title: fig.suptitle(title, y=1.02)
    plt.tight_layout(); plt.show()

print("\nExamples of label 0 (No Cancer):")
plot_tiles(sample_ids(df, 0, 12), title="Label 0 Samples")

print("\nExamples of label 1 (Cancer):")
plot_tiles(sample_ids(df, 1, 12), title="Label 1 Samples")

"""### Step 4 - Visualizing Regions of Interest (ROI)

To gain a deeper understanding of what spatial information the model might learn, I visualized the central **Region of Interest (ROI)** inside randomly selected tiles from each class.  
Each tile is 96×96 pixels, and I overlaid a dashed red bounding box of size 32×32 pixels centered in the image.

- This helps highlight where most clinically relevant morphological patterns lie  
- Many tiles contain dense cellular regions in the center even when the background contains empty space  
- The ROI often contains nuclei-dense structures that differentiate healthy vs. metastatic tissue

**Observations:**
- Label **0 (no metastasis)** samples tend to show more uniform cellular spacing and regular nuclei arrangements
- Label **1 (cancer)** samples often display crowded, irregular nuclei and disrupted tissue structure
- Some tiles contain a mix of tissue and background, which increases dataset variability
- ROI visualization reinforces why convolutional layers and small kernels are helpful for detecting fine-scale patterns

The images below show representative examples from both classes with ROIs highlighted.

"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import math

def plot_tiles_with_roi(ids, root=TRAIN_DIR, box_size=32, ncols=6, title=None):
    """
    Display image tiles with a highlighted Region of Interest (ROI) drawn as a red dashed box.
    Args:
        ids       : list of image IDs (without .tif)
        root      : directory containing the .tif files
        box_size  : side length of the ROI box (square)
        ncols     : number of columns in the figure
        title     : optional title for the entire figure
    """
    if len(ids) == 0:
        print("No images to visualize.")
        return

    nrows = math.ceil(len(ids) / ncols)
    fig, axes = plt.subplots(nrows, ncols, figsize=(2*ncols, 2*nrows))
    axes = np.array(axes).reshape(-1)

    for ax, img_id in zip(axes, ids):
        img_path = str(root / f"{img_id}.tif")
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)

        if img is None:
            ax.set_title(f"{img_id} (missing)")
            ax.axis("off")
            continue

        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        ax.imshow(img)
        ax.set_title(str(img_id), fontsize=8)
        ax.axis("off")

        # --- draw center ROI box ---
        H, W, _ = img.shape
        bs = box_size
        x0 = (W - bs) // 2
        y0 = (H - bs) // 2

        rect = Rectangle((x0, y0), bs, bs,
                         linewidth=1.8,
                         edgecolor="red",
                         linestyle="--",
                         facecolor="none")
        ax.add_patch(rect)

    if title:
        fig.suptitle(title, fontsize=12)

    plt.tight_layout()
    plt.show()


# Example usage (just like your other EDA figures):
ids_class0 = sample_ids(df, lbl=0, k=6)
ids_class1 = sample_ids(df, lbl=1, k=6)

print("Examples from Class 0 (No Cancer) with ROI:")
plot_tiles_with_roi(ids_class0, box_size=32, title="Label 0 Samples with ROI")

print("Examples from Class 1 (Cancer) with ROI:")
plot_tiles_with_roi(ids_class1, box_size=32, title="Label 1 Samples with ROI")

# === EDA: RGB histograms and simple Reinhard-like color normalization demo ===
import cv2, numpy as np, matplotlib.pyplot as plt
from pathlib import Path
from glob import glob

SAMPLE_IDS = list(df['id'].sample(512, random_state=42))  # df = train labels DataFrame
sample_paths = [str(TRAIN_DIR / f"{i}.tif") for i in SAMPLE_IDS]

def read_rgb(p):
    img = cv2.imread(p, cv2.IMREAD_COLOR)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    return img

# RGB histograms
all_pixels = []
for p in sample_paths:
    all_pixels.append(read_rgb(p).reshape(-1,3))
all_pixels = np.concatenate(all_pixels, axis=0)

fig, axs = plt.subplots(1,3, figsize=(12,3))
ch_names = ['R','G','B']
for c in range(3):
    axs[c].hist(all_pixels[:,c], bins=64, density=True)
    axs[c].set_title(f'{ch_names[c]}-channel intensity')
plt.tight_layout(); plt.show()

# Simple color normalization demo (per-image mean/std to global mean/std)
global_mean = all_pixels.mean(axis=0)
global_std  = all_pixels.std(axis=0) + 1e-6

def simple_color_norm(img):
    m = img.reshape(-1,3).mean(axis=0)
    s = img.reshape(-1,3).std(axis=0) + 1e-6
    norm = (img - m) / s * global_std + global_mean
    return np.clip(norm, 0, 255).astype(np.uint8)

# visualize a few before/after
fig, axes = plt.subplots(2,6, figsize=(12,4))
for k, p in enumerate(sample_paths[:6]):
    raw = read_rgb(p)
    nor = simple_color_norm(raw)
    axes[0,k].imshow(raw); axes[0,k].axis('off'); axes[0,k].set_title('Raw')
    axes[1,k].imshow(nor); axes[1,k].axis('off'); axes[1,k].set_title('Norm')
plt.tight_layout(); plt.show()

"""### Step 5 - Train/Validation Split (Stratified)

Before model training, the dataset is divided into training and validation subsets.  
Because the original dataset is imbalanced, a **stratified 80/20 split** is applied to ensure that both subsets preserve the same class distribution.

Keras `flow_from_dataframe()` requires filenames, so a new `filename` column is created by attaching `.tif` to each image ID.  
Additionally, labels are converted to string type because Keras generators expect categorical values as strings.

After this step:
- `train_df` → used for model training  
- `val_df` → used for validation
- Class balance is preserved

"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# convert filename column
df["filename"] = df["id"].astype(str) + ".tif"

# stratified 80/20 split
train_df, val_df = train_test_split(
    df[["filename","label"]],
    test_size=0.20,
    random_state=2025,
    stratify=df["label"]
)

# Keras requires string labels
train_df["label"] = train_df["label"].astype(str)
val_df["label"]   = val_df["label"].astype(str)

print("Train:", len(train_df), "Validation:", len(val_df))

# === Pre-filter: remove near-blank tiles by tissue fraction ===
import numpy as np, cv2

def tissue_fraction(img, thr=210):
    # count pixels darker than threshold (rough proxy for tissue)
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    return (gray < thr).mean()

KEEP_IDS = []
for _id in df['id']:
    p = str(TRAIN_DIR / f"{_id}.tif")
    img = cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)
    if tissue_fraction(img) >= 0.02:   # keep if ≥2% tissue
        KEEP_IDS.append(_id)

df_filt = df[df['id'].isin(KEEP_IDS)].reset_index(drop=True)
print(f"Kept {len(df_filt):,} / {len(df):,} tiles after tissue filter")
# use df_filt for train/val split from now on

"""### Step 6 - Data Generators and Augmentation

To feed image batches into our neural network efficiently, we use Keras `ImageDataGenerator`.  
Since histopathology images can contain rotation and orientation variance, data augmentation is applied to improve generalization and reduce overfitting.

**Training generator includes:**
- Rescaling pixel values to `[0,1]`
- Random horizontal & vertical flips
- Small rotations (±15°)
- Width and height shifts
- Mild brightness changes
- Zoom augmentation
- `fill_mode="nearest"` to preserve edges

The validation generator only rescales images, ensuring that evaluation is performed on unmodified data.

Two generators are created:
- `train_gen` – produces shuffled augmented batches for training
- `val_gen` – produces batches for evaluation (no augmentation)

"""

IMG_SIZE = (96, 96)
BATCH    = 8

train_datagen = ImageDataGenerator(
    rescale=1./255.,
    horizontal_flip=True,
    vertical_flip=True,
    rotation_range=15,
    width_shift_range=0.05,
    height_shift_range=0.05,
    brightness_range=(0.92,1.08),
    zoom_range=0.10,
    fill_mode="nearest"
)

val_datagen = ImageDataGenerator(rescale=1./255.)

train_gen = train_datagen.flow_from_dataframe(
    train_df,
    directory=str(TRAIN_DIR),
    x_col="filename",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH,
    class_mode="binary",
    shuffle=True
)

val_gen = val_datagen.flow_from_dataframe(
    val_df,
    directory=str(TRAIN_DIR),
    x_col="filename",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH,
    class_mode="binary",
    shuffle=False
)

"""### Step 7 - Baseline CNN (my sequential 96×96 design)

I built a compact yet regularized 2D CNN tailored to 96×96 RGB tiles. The network stacks three convolutional blocks followed by a lightweight classifier head. My design goals were: (i) strong inductive bias for texture/structure typical of H&E slides, (ii) low parameter count for stable training on modest hardware, and (iii) explicit regularization to curb overfitting.

**Architecture (Sequential):**
- **Input:** `(96, 96, 3)` RGB tile.
- **Block 1:** `Conv(16, 3×3, same, no bias) → BN → ReLU → Conv(16, 3×3, same, no bias) → BN → ReLU → MaxPool(2) → Spatial Dropout(0.30)`.
- **Block 2:** `Conv(16, 3×3, same, no bias) → BN → ReLU → MaxPool(2) → Dropout(0.30)`.
- **Block 3:** `Conv(32, 3×3, same, no bias) → BN → ReLU → MaxPool(2) → Dropout(0.30)`.
- **Head:** either `Flatten` (used here) or `GlobalAveragePooling2D` (parameter-efficient alternative) → `Dense(32, ReLU)` → `Dropout(0.25)` → `Dense(1, Sigmoid)`.

**Regularization & stabilization choices:**
- **L2 weight decay:** `1e-4` on all conv/dense kernels to penalize overly complex filters.
- **Batch Normalization:** after each convolution to stabilize feature distributions and allow higher learning rates.
- **Dropout / SpatialDropout:** 0.30 in conv blocks and 0.25 in the head to reduce co-adaptation; spatial variant combats overconfident channel activations.
- **No biases in conv layers:** BN absorbs the shift, removing redundant parameters.

**Why this works for histopathology tiles:**
- Two stacked 3×3 convolutions per block increase the effective receptive field while maintaining locality—well-suited for nuclei morphology and glandular textures.
- Progressive pooling (×8 downsampling overall) captures meso-scale context without destroying cellular detail.
- The shallow-to-moderate depth (16→32 channels) balances signal capacity with overfitting risk on 96×96 crops.

**Head choice (Flatten vs GAP):**
- I use **Flatten + Dense(32)** to give the classifier a small capacity bump; switching to **GAP** is a one-line change if I need fewer parameters or stronger translation invariance.

**Output:**
- A single **sigmoid** node yields the posterior $$ \hat{p}(y=1 \mid x) $$, enabling AUC/precision/recall monitoring during training.

I keep this model as a strong, fast baseline; more expressive variants (deeper width, double-conv blocks, or pretrained backbones) are compared later under identical augmentation and evaluation protocols.

"""

# === Model: Sequential CNN for 96×96 tiles (build only) ===
from tensorflow.keras import Sequential, layers, regularizers

# ---- hyperparameters ----
IMG_SHAPE   = (96, 96, 3)
l1, l2, l3  = 16, 16, 32
KERNEL      = 3
PADDING     = "same"
POOL        = 2
DROPOUT     = 0.30
L2W         = 1e-4
USE_FLATTEN = True   # set False to use GlobalAveragePooling2D (fewer params)

model = Sequential(name="seq_cnn_96")
model.add(layers.Input(shape=IMG_SHAPE))

# block 1 (two convs)
model.add(layers.Conv2D(l1, KERNEL, padding=PADDING, use_bias=False,
                        kernel_regularizer=regularizers.l2(L2W)))
model.add(layers.BatchNormalization()); model.add(layers.Activation("relu"))
model.add(layers.Conv2D(l1, KERNEL, padding=PADDING, use_bias=False,
                        kernel_regularizer=regularizers.l2(L2W)))
model.add(layers.BatchNormalization()); model.add(layers.Activation("relu"))
model.add(layers.MaxPooling2D(pool_size=POOL)); model.add(layers.Dropout(DROPOUT))

# block 2
model.add(layers.Conv2D(l2, KERNEL, padding=PADDING, use_bias=False,
                        kernel_regularizer=regularizers.l2(L2W)))
model.add(layers.BatchNormalization()); model.add(layers.Activation("relu"))
model.add(layers.MaxPooling2D(pool_size=POOL)); model.add(layers.Dropout(DROPOUT))

# block 3
model.add(layers.Conv2D(l3, KERNEL, padding=PADDING, use_bias=False,
                        kernel_regularizer=regularizers.l2(L2W)))
model.add(layers.BatchNormalization()); model.add(layers.Activation("relu"))
model.add(layers.MaxPooling2D(pool_size=POOL)); model.add(layers.Dropout(DROPOUT))

# classifier head
if USE_FLATTEN:
    model.add(layers.Flatten())
else:
    model.add(layers.GlobalAveragePooling2D())
model.add(layers.Dense(32, activation="relu", kernel_regularizer=regularizers.l2(L2W)))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(1, activation="sigmoid"))

model.summary()

"""### Step 7.1 - Model Visualization and Summary Export

To clearly document the architecture of my baseline convolutional network, I exported two useful artifacts:

1. **`model_architecture.png`**  
   - A structural diagram showing all layers, their connections, and tensor shapes.

2. **`model_summary.png`**  
   - A complete layer-by-layer summary including output shapes, parameter counts, and trainable vs. non-trainable parameters.

These files make the model easy to interpret in the report and are also included in my GitHub repository for reproducibility.

"""

from tensorflow.keras.utils import plot_model
from io import StringIO
import matplotlib.pyplot as plt

# diagram
plot_model(model, to_file="model_architecture.png",
           show_shapes=True, show_layer_names=True, dpi=160, expand_nested=True)

# summary table image
buf = StringIO()
model.summary(print_fn=lambda s: buf.write(s + "\n"))
txt = buf.getvalue()
plt.figure(figsize=(9, 12)); plt.axis("off")
plt.text(0, 1, txt, family="monospace", fontsize=8, va="top")
plt.savefig("model_summary.png", dpi=200, bbox_inches="tight"); plt.show()

print(" saved: model_architecture.png | model_summary.png")

"""### Step 8 - Model Training, Class Imbalance Handling, and Evaluation

To train the baseline CNN, I compiled the model using **binary cross-entropy** with an **Adam optimizer** and tracked several performance metrics: AUC, accuracy, precision, and recall.  
Because the dataset is moderately imbalanced (more negative samples than positive), I computed **class weights** and passed them to `model.fit()` to prevent the model from favoring the majority class.

During training, I used three callbacks:

- **ModelCheckpoint** — automatically stores the best model based on validation AUC.
- **EarlyStopping** — stops training if validation AUC stops improving and restores the best weights.
- **ReduceLROnPlateau** — reduces the learning rate when validation AUC plateaus, helping the model converge more smoothly.

The model was trained for up to 12 epochs using augmented batches from the training generator.  
After training, I evaluated the best restored model on the validation set and explicitly printed key metrics, including **Accuracy**, **Precision**, and **Recall**.  
I also saved the training history to `train_history.json` for reproducibility and visualization later.

"""

# === Train: compile + callbacks + fit (show "accuracy") ===
from tensorflow import keras
from tensorflow.keras import callbacks, optimizers, metrics
import numpy as np, json

# class weights (imbalance-safe)
u, c = np.unique(train_gen.classes, return_counts=True)
tot = int(c.sum())
class_weight = {int(k): float(tot / (2*max(v,1))) for k, v in zip(u, c)}
print("class_weight:", class_weight)

# compile with accuracy label
model.compile(
    optimizer=optimizers.Adam(1e-3),
    loss="binary_crossentropy",
    metrics=[
        metrics.AUC(name="auc"),
        metrics.BinaryAccuracy(name="accuracy", threshold=0.5),  # <-- spelled out
        metrics.Precision(name="prec"),
        metrics.Recall(name="rec"),
    ]
)

cbs = [
    callbacks.ModelCheckpoint("seq_cnn96_best.keras", monitor="val_auc", mode="max",
                              save_best_only=True, verbose=1),
    callbacks.EarlyStopping(monitor="val_auc", mode="max",
                            patience=3, restore_best_weights=True, verbose=1),
    callbacks.ReduceLROnPlateau(monitor="val_auc", mode="max",
                                factor=0.5, patience=2, min_lr=1e-6, verbose=1),
]

history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=12,
    class_weight=class_weight,
    callbacks=cbs,
    verbose=1
)

print(" best weights saved: seq_cnn96_best.keras")

# evaluation prints "Accuracy" explicitly
val_loss, val_auc, val_accuracy, val_prec, val_rec = model.evaluate(val_gen, verbose=0)
print(f"Validation — loss: {val_loss:.4f} | AUC: {val_auc:.4f} | "
      f"Accuracy: {val_accuracy:.4f} | Precision: {val_prec:.4f} | Recall: {val_rec:.4f}")

# save history
with open("train_history.json", "w") as f:
    json.dump(history.history, f)

"""### Step 9 — Training & Validation Curve Visualization

To better understand how my CNN behaved during training, I visualized the **training vs. validation accuracy** and **training vs. validation loss** across epochs.

After loading the saved `train_history.json` (or using the in-memory `history` object), I extracted accuracy and loss values for each epoch. I plotted them side-by-side:

- The **left plot** shows how the model’s accuracy improved over epochs for both training and validation sets.
- The **right plot** shows the corresponding **binary cross-entropy loss**, which helps indicate whether the model is overfitting or underfitting.

These visualizations allow me to see:
- Whether accuracy keeps improving  
- Whether validation accuracy tracks training accuracy  
- If loss diverges, signaling overfitting  
- How many epochs are optimal before performance plateaus

Finally, I saved the figure as `training_curves_accuracy_loss.png` so it can be included in the report and GitHub repository.

"""

# === Plots: Training vs Validation (Accuracy & Loss) ===
import json, matplotlib.pyplot as plt

# pull metrics from in-memory history if available; else from JSON
hist = None
try:
    hist = history.history
except NameError:
    pass

if hist is None:
    with open("train_history.json", "r") as f:
        hist = json.load(f)

# handle both 'accuracy'/'val_accuracy' and legacy 'acc'/'val_acc'
train_acc = hist.get("accuracy", hist.get("acc", []))
val_acc   = hist.get("val_accuracy", hist.get("val_acc", []))
train_loss = hist.get("loss", [])
val_loss   = hist.get("val_loss", [])

epochs = range(1, len(train_loss) + 1)

plt.figure(figsize=(12,5))

# Accuracy
plt.subplot(1,2,1)
plt.plot(epochs, train_acc, "-o", label="Training Accuracy")
plt.plot(epochs, val_acc, "-o", label="Validation Accuracy")
plt.xlabel("Epoch"); plt.ylabel("Accuracy"); plt.title("Accuracy per Epoch")
plt.grid(True); plt.legend()

# Loss
plt.subplot(1,2,2)
plt.plot(epochs, train_loss, "-o", label="Training Loss")
plt.plot(epochs, val_loss, "-o", label="Validation Loss")
plt.xlabel("Epoch"); plt.ylabel("Binary Crossentropy"); plt.title("Loss per Epoch")
plt.grid(True); plt.legend()

plt.tight_layout()
plt.savefig("training_curves_accuracy_loss.png", dpi=160, bbox_inches="tight")
plt.show()

print("saved: training_curves_accuracy_loss.png")

"""### Step 10 - Double Convolution Architecture — Model Training & Evaluation

To improve feature extraction compared to my first baseline CNN, I implemented a **Double Convolution architecture**.  
In this design, each block contains two consecutive convolution layers followed by **Batch Normalization** and **ReLU activation**. Stacking two convolutions per block allows the network to learn richer texture patterns, which is especially useful in histopathology images where small morphological differences matter.

The model has three such double-conv blocks with filter sizes **32 → 64 → 128**, followed by `MaxPooling`, `GlobalAveragePooling`, and a final sigmoid output neuron for binary classification.

I also used **class weighting** to address dataset imbalance, since the cancer (label 1) class is smaller than the non-cancer class. To stabilize training and reduce overfitting risk, I added:
- EarlyStopping (restores the best epoch)
- ModelCheckpoint (saves best weights based on validation AUC)
- ReduceLROnPlateau (lowers learning rate if progress stalls)

After training for up to 12 epochs, the model achieved strong validation performance, and I explicitly printed **Accuracy**, **AUC**, **Precision**, and **Recall** to make interpretation straightforward.

Finally, I saved both the best model (`doubleconv96_best.keras`) and the training history (`doubleconv_train_history.json`) so I can reuse them later for comparison and plots.

"""

# === Double Convolution Layer Architecture: build + train + evaluate (prints Accuracy) ===
from tensorflow import keras
from tensorflow.keras import layers, callbacks, optimizers, metrics
import numpy as np, json

# ----- model -----
def double_conv_block(x, filters):
    x = layers.Conv2D(filters, 3, padding="same", use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.Conv2D(filters, 3, padding="same", use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    return x

inp = keras.Input(shape=(96, 96, 3))
x = double_conv_block(inp, 32)
x = layers.MaxPooling2D(2)(x)

x = double_conv_block(x, 64)
x = layers.MaxPooling2D(2)(x)

x = double_conv_block(x, 128)
x = layers.MaxPooling2D(2)(x)

x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.35)(x)
out = layers.Dense(1, activation="sigmoid")(x)

model_double = keras.Model(inp, out, name="double_conv_96")
model_double.summary()

# ----- class weights (handles imbalance) -----
u, c = np.unique(train_gen.classes, return_counts=True)
tot = int(c.sum())
class_weight = {int(k): float(tot/(2*max(v,1))) for k, v in zip(u, c)}
print("class_weight:", class_weight)

# ----- compile -----
model_double.compile(
    optimizer=optimizers.Adam(1e-3),
    loss="binary_crossentropy",
    metrics=[
        metrics.AUC(name="auc"),
        metrics.BinaryAccuracy(name="accuracy", threshold=0.5),  # Accuracy shown explicitly
        metrics.Precision(name="precision"),
        metrics.Recall(name="recall"),
    ],
)

# ----- callbacks -----
cbs = [
    callbacks.ModelCheckpoint("doubleconv96_best.keras", monitor="val_auc", mode="max",
                              save_best_only=True, verbose=1),
    callbacks.EarlyStopping(monitor="val_auc", mode="max",
                            patience=3, restore_best_weights=True, verbose=1),
    callbacks.ReduceLROnPlateau(monitor="val_auc", mode="max",
                                factor=0.5, patience=2, min_lr=1e-6, verbose=1),
]

# ----- train -----
history_double = model_double.fit(
    train_gen,
    validation_data=val_gen,
    epochs=12,
    class_weight=class_weight,
    callbacks=cbs,
    verbose=1
)

# ----- evaluate (prints Accuracy explicitly) -----
val_loss, val_auc, val_acc, val_prec, val_rec = model_double.evaluate(val_gen, verbose=0)
print(f"\n Validation Results (DoubleConv):")
print(f"  Loss      : {val_loss:.4f}")
print(f"  AUC       : {val_auc:.4f}")
print(f"  Accuracy  : {val_acc:.4f}")      # <-- Accuracy shown clearly
print(f"  Precision : {val_prec:.4f}")
print(f"  Recall    : {val_rec:.4f}")

# (optional) save history for plots
with open("doubleconv_train_history.json", "w") as f:
    json.dump(history_double.history, f)
print("Saved: doubleconv96_best.keras, doubleconv_train_history.json")

"""### Step 11 — Plotting Training Curves for the Double Convolution Model

In this step, I visualized the learning behavior of the Double Convolution architecture.  
After training the model, I extracted the training history (accuracy, loss, and AUC) and plotted the corresponding curves.

- **Accuracy Plot:** Shows how well the model learns to classify cancer vs. non-cancer patches over epochs.  
- **Loss Plot:** Displays reduction in binary cross-entropy loss, indicating improved model confidence and prediction quality.  
- **AUC Plot:** Since cancer detection is a highly imbalanced classification problem, AUC is a strong indicator of performance.  
  Higher AUC means better separation between positive and negative samples.

These visualizations allow me to directly compare training and validation behavior, monitor overfitting or underfitting, and confirm that the DoubleConv model converged properly.

After running the code below, all three plots are generated successfully.

"""

# === Plot Training Curves for Double Convolution Model (Accuracy, Loss, AUC) ===
import json
import matplotlib.pyplot as plt

# --- load training history ---
if 'history_double' in globals():
    hist = history_double.history
else:
    with open("doubleconv_train_history.json", "r") as f:
        hist = json.load(f)

# extract metrics
train_acc = hist.get("accuracy", [])
val_acc   = hist.get("val_accuracy", [])
train_loss = hist.get("loss", [])
val_loss   = hist.get("val_loss", [])
train_auc  = hist.get("auc", [])
val_auc    = hist.get("val_auc", [])

epochs = range(1, len(train_loss) + 1)

# ---- Accuracy ----
plt.figure(figsize=(6,4))
plt.plot(epochs, train_acc, 'o-', label="Train Accuracy")
plt.plot(epochs, val_acc, 'o-', label="Val Accuracy")
plt.title("DoubleConv — Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.grid(True)
plt.legend()
plt.show()

# ---- Loss ----
plt.figure(figsize=(6,4))
plt.plot(epochs, train_loss, 'o-', label="Train Loss")
plt.plot(epochs, val_loss, 'o-', label="Val Loss")
plt.title("DoubleConv — Loss")
plt.xlabel("Epoch")
plt.ylabel("Binary Crossentropy Loss")
plt.grid(True)
plt.legend()
plt.show()

# ---- AUC ----
plt.figure(figsize=(6,4))
plt.plot(epochs, train_auc, 'o-', label="Train AUC")
plt.plot(epochs, val_auc, 'o-', label="Val AUC")
plt.title("DoubleConv — AUC")
plt.xlabel("Epoch")
plt.ylabel("AUC")
plt.grid(True)
plt.legend()
plt.show()

print(" Plots for Double Convolution model generated successfully.")

"""### Step 12 — Double-Deep CNN (Build, Train, Evaluate)

In this step, I implemented a deeper convolutional model using a *double-conv* block at every stage.  
Each stage applies two consecutive Conv+BatchNorm+ReLU layers, followed by MaxPooling and SpatialDropout to reduce overfitting.

✔ Model depth: 4 convolutional stages  
✔ Feature sizes: 32 → 64 → 128 → 256  
✔ Uses Global Average Pooling instead of Flatten (reduces parameters and helps generalization)  
✔ Two dense layers with dropout before the final sigmoid output

To address the dataset imbalance, I computed **class weights** and passed them into `model.fit()`, so the model does not ignore the minority class.

I monitored validation AUC and used:
- `ModelCheckpoint` → saves the best model
- `EarlyStopping` → stops training when AUC stops improving
- `ReduceLROnPlateau` → lowers learning rate when progress stalls

After training, I evaluated the model and printed the key metrics:  
**Loss, AUC, Accuracy, Precision, and Recall**, so I can compare this architecture directly with earlier models.

Finally, I also saved the training history to a JSON file for visualization later.

"""

# === Double Deep Architecture (build + train + evaluate; prints Accuracy) ===
from tensorflow import keras
from tensorflow.keras import layers, callbacks, optimizers, metrics
import numpy as np, json

# ----- double-conv block -----
def double_conv(x, f):
    x = layers.Conv2D(f, 3, padding="same", use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.Conv2D(f, 3, padding="same", use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    return x

# ===== Model: Deeper 4-stage CNN with double-conv per stage =====
inp = keras.Input(shape=(96, 96, 3))

x = double_conv(inp, 32);  x = layers.MaxPooling2D(2)(x);  x = layers.SpatialDropout2D(0.10)(x)
x = double_conv(x,  64);   x = layers.MaxPooling2D(2)(x);  x = layers.SpatialDropout2D(0.10)(x)
x = double_conv(x, 128);   x = layers.MaxPooling2D(2)(x);  x = layers.SpatialDropout2D(0.15)(x)
x = double_conv(x, 256);   x = layers.MaxPooling2D(2)(x);  x = layers.SpatialDropout2D(0.20)(x)

x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.35)(x)
x = layers.Dense(128, activation="relu")(x)
x = layers.Dropout(0.25)(x)
out = layers.Dense(1, activation="sigmoid", dtype="float32")(x)  # ensures stable metric dtype

model_doubledeep = keras.Model(inp, out, name="double_deep_96")
model_doubledeep.summary()

# ===== Class weights (handles imbalance) =====
u, c = np.unique(train_gen.classes, return_counts=True)
tot = int(c.sum())
class_weight = {int(k): float(tot / (2*max(v,1))) for k, v in zip(u, c)}
print("class_weight:", class_weight)

# ===== Compile =====
model_doubledeep.compile(
    optimizer=optimizers.Adam(1e-3),
    loss="binary_crossentropy",
    metrics=[
        metrics.AUC(name="auc"),
        metrics.BinaryAccuracy(name="accuracy", threshold=0.5),  # Accuracy shown explicitly
        metrics.Precision(name="precision"),
        metrics.Recall(name="recall"),
    ],
)

# ===== Callbacks =====
cbs = [
    callbacks.ModelCheckpoint("doubledeep96_best.keras", monitor="val_auc", mode="max",
                              save_best_only=True, verbose=1),
    callbacks.EarlyStopping(monitor="val_auc", mode="max",
                            patience=4, restore_best_weights=True, verbose=1),
    callbacks.ReduceLROnPlateau(monitor="val_auc", mode="max",
                                factor=0.5, patience=2, min_lr=1e-6, verbose=1),
]

# ===== Train =====
history_doubledeep = model_doubledeep.fit(
    train_gen,
    validation_data=val_gen,
    epochs=12,
    class_weight=class_weight,
    callbacks=cbs,
    verbose=1
)

# ===== Evaluate (prints Accuracy clearly) =====
val_loss, val_auc, val_acc, val_prec, val_rec = model_doubledeep.evaluate(val_gen, verbose=0)
print("\n Validation Results (Double Deep):")
print(f"  Loss      : {val_loss:.4f}")
print(f"  AUC       : {val_auc:.4f}")
print(f"  Accuracy  : {val_acc:.4f}")      # <-- Accuracy explicitly
print(f"  Precision : {val_prec:.4f}")
print(f"  Recall    : {val_rec:.4f}")

# (optional) save history for plots
with open("doubledeep_train_history.json", "w") as f:
    json.dump(history_doubledeep.history, f)
print("Saved: doubledeep96_best.keras, doubledeep_train_history.json")

"""### Step 13 - Training Curves — Double Deep Architecture

To analyze the learning behavior of the **Double Deep CNN**, we visualize the evolution of three key metrics across training epochs: **Accuracy**, **Binary Cross-Entropy Loss**, and **AUC**. These curves allow us to check convergence behavior, potential overfitting, and overall generalization performance.

####  Accuracy (Train vs Validation)

The validation accuracy consistently improves over epochs and stays close to the training accuracy, indicating **good generalization** and **no severe overfitting**.  
By the final epoch, both curves stabilize around ~0.92–0.94.

####  Loss (Train vs Validation)

The training and validation loss both decrease smoothly, confirming stable optimization.  
There is a small spike at one epoch in the validation curve, likely caused by a noisy batch or random variation in validation sampling, but the loss recovers immediately and continues downward.

####  AUC (Train vs Validation)

AUC steadily increases for both training and validation sets.  
Validation AUC remains above ~0.96 in later epochs, demonstrating strong **ranking ability** and reliable separation between positive and negative classes.

Overall, these curves show that the **Double Deep model** is learning effectively, converging smoothly, and maintaining strong generalization performance without significant overfitting.

"""

# === Plots for Double Deep Architecture (Accuracy, Loss, AUC) ===
import json
import matplotlib.pyplot as plt

# --- load history from memory or file ---
if 'history_doubledeep' in globals():
    hist = history_doubledeep.history
else:
    with open("doubledeep_train_history.json", "r") as f:
        hist = json.load(f)

# extract metrics if present
train_acc  = hist.get("accuracy", [])
val_acc    = hist.get("val_accuracy", [])
train_loss = hist.get("loss", [])
val_loss   = hist.get("val_loss", [])
train_auc  = hist.get("auc", [])
val_auc    = hist.get("val_auc", [])

epochs = range(1, len(train_loss) + 1)

# ---- Accuracy ----
plt.figure(figsize=(6,4))
plt.plot(epochs, train_acc, 'o-', label="Train Accuracy")
plt.plot(epochs, val_acc,   'o-', label="Val Accuracy")
plt.title("Double Deep — Accuracy")
plt.xlabel("Epoch"); plt.ylabel("Accuracy")
plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()

# ---- Loss ----
plt.figure(figsize=(6,4))
plt.plot(epochs, train_loss, 'o-', label="Train Loss")
plt.plot(epochs, val_loss,   'o-', label="Val Loss")
plt.title("Double Deep — Loss")
plt.xlabel("Epoch"); plt.ylabel("Binary Crossentropy Loss")
plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()

# ---- AUC ----
plt.figure(figsize=(6,4))
plt.plot(epochs, train_auc, 'o-', label="Train AUC")
plt.plot(epochs, val_auc,   'o-', label="Val AUC")
plt.title("Double Deep — AUC")
plt.xlabel("Epoch"); plt.ylabel("AUC")
plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()

print(" Plots generated for Double Deep model.")

"""### Step 14 - Class Weight Computation

Because the dataset is imbalanced, I computed class weights using the training labels from the data generator. By applying `compute_class_weight` with the balanced strategy, I obtained a weight for each class `{0: w0, 1: w1}`. I then passed this `CLASS_WEIGHT` dictionary to the model during training to reduce bias toward the majority class and ensure both cancer and non-cancer samples influence learning fairly.

"""

# === Compute class weights for imbalance ===
from sklearn.utils import class_weight
import numpy as np

# y_train: labels of training set (0/1). If you have df_filt, use that.
y_train = train_gen.classes  # works for Keras generators

# compute weights: dictionary {0: w0, 1: w1}
cw = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
CLASS_WEIGHT = {i: w for i, w in enumerate(cw)}
print("CLASS_WEIGHT =", CLASS_WEIGHT)

"""### Step 15 - Pretrained Baseline — EfficientNetB0 @ 96×96

**Setup.** I used `EfficientNetB0` (ImageNet weights) as a pretrained baseline at input size **96×96**. I first **froze** the backbone for a short warm-up, then **unfroze** it and fine-tuned with a smaller learning rate. I kept the same metrics as other models and used the previously computed `CLASS_WEIGHT`.

**Training protocol.**
1. **Warm-up (frozen backbone, 3 epochs):** `Adam(1e-3)`, BCE loss, metrics = AUC/Acc/Precision/Recall.
2. **Fine-tuning (unfrozen, 5 epochs):** `Adam(1e-4)`, same loss/metrics.

**Key results (validation).**
- **Warm-up:** AUC ≈ **0.556** (low, as expected when the backbone is frozen).
- **Fine-tune:** AUC per epoch = **0.8196**, 0.5855, **0.9182** (best), 0.4727, 0.5672.  
  Best epoch (fine-tune **#3**) reached **Val AUC ≈ 0.918** and **Val Acc ≈ 0.854**.
- The curves show **volatility across epochs** (e.g., epoch #4 dip), so I select the **best epoch by Val AUC** (saved via `ModelCheckpoint`) for downstream evaluation and submission.

**Conclusion.** With brief warm-up + targeted fine-tuning, the pretrained EffNetB0 yields a strong **best Val AUC ≈ 0.918** at 96×96. I use that best checkpoint as the EfficientNet baseline in the final comparison.

"""

# === Pretrained baseline: EfficientNetB0 on 96x96 ===
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.applications import EfficientNetB0

# If not already defined in your notebook:
try:
    IMG_SHAPE
except NameError:
    IMG_SHAPE = (96, 96, 3)

# 1) Build backbone (ImageNet weights) and freeze for warm-up
base = EfficientNetB0(include_top=False, weights='imagenet', input_shape=IMG_SHAPE)
base.trainable = False

inp = layers.Input(shape=IMG_SHAPE)
x   = base(inp, training=False)
x   = layers.GlobalAveragePooling2D()(x)
x   = layers.Dropout(0.25)(x)
out = layers.Dense(1, activation='sigmoid')(x)
model_eff = Model(inp, out, name='effb0_96')

# 2) Compile (use same metrics you used elsewhere)
model_eff.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss='binary_crossentropy',
    metrics=[tf.keras.metrics.AUC(name='auc'), 'accuracy',
             tf.keras.metrics.Precision(name='precision'),
             tf.keras.metrics.Recall(name='recall')]
)

# 3) Warm-up training with frozen base
hist_eff_warm = model_eff.fit(
    train_gen,
    validation_data=val_gen,
    epochs=3,                      # short warm-up
    class_weight=CLASS_WEIGHT,     # keep your class weights
    callbacks=[early_stop, ckpt_cb] if 'early_stop' in globals() and 'ckpt_cb' in globals() else None,
    verbose=1
)

# 4) Unfreeze and fine-tune with smaller LR
base.trainable = True
model_eff.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='binary_crossentropy',
    metrics=[tf.keras.metrics.AUC(name='auc'), 'accuracy',
             tf.keras.metrics.Precision(name='precision'),
             tf.keras.metrics.Recall(name='recall')]
)
hist_eff_ft = model_eff.fit(
    train_gen,
    validation_data=val_gen,
    epochs=5,                      # adjust if needed
    class_weight=CLASS_WEIGHT,
    callbacks=[early_stop, ckpt_cb] if 'early_stop' in globals() and 'ckpt_cb' in globals() else None,
    verbose=1
)

# 5) Optional: evaluate on validation for quick summary
val_metrics = model_eff.evaluate(val_gen, verbose=0)
print("EffB0 (fine-tuned) - val metrics:", dict(zip(model_eff.metrics_names, val_metrics)))

# 6) (Optional) set as a candidate best_model for later sections
best_model = model_eff  # if you want to use this as the deployment model in later steps

"""### Step 16 -  Model Selection Summary — Best Epoch & Validation Metrics

I selected the **best model/epoch** by prioritizing validation metrics in the order  
**Val AUC → Val AUPRC → Val Accuracy** (fallback to the last epoch if absent).  
Among all trained candidates (Basic, Double Conv, Double Deep, EfficientNet variants), the winner is:

**Best Model:** *Double Deep* — **Epoch 12**

**Validation metrics (bar chart):**
- **AUC:** 0.983  
- **Accuracy:** 0.941  
- **Precision:** 0.966  
- **Recall:** 0.886

**Training snapshot (same epoch):**  
AUC = 0.975, Accuracy = 0.927, Precision = 0.966, Recall = 0.914, Loss = 0.198

**Validation snapshot:**  
AUC = 0.983, Accuracy = 0.941, Precision = 0.966, Recall = 0.886, Loss = 0.160

**Interpretation.**  
The Double Deep model at epoch 12 achieves the highest **Val AUC** while maintaining a strong precision–recall balance. The gap between train and validation is modest, indicating good generalization. I use this checkpoint as the reference model for downstream evaluation (threshold selection, error analysis) and for generating the Kaggle submission.

"""

# === Conclusion: pick best model/epoch and plot its metrics (more robust) ===
import json, os
import numpy as np
import matplotlib.pyplot as plt

def _ensure_hist(h):
    if hasattr(h, "history"):
        return h.history
    elif isinstance(h, dict):
        return h
    else:
        raise ValueError("History must be a Keras History or a dict of lists.")

def _pick_first_metric(hdict, candidates):
    """
    Return (key, array) for the first available validation metric in candidates.
    Example candidates: ['val_auc', 'val_auprc', 'val_pr_auc', 'val_accuracy']
    """
    for k in candidates:
        if k in hdict and len(hdict[k]) > 0:
            return k, hdict[k]
    return None, None

def _best_epoch_idx(hist):
    h = _ensure_hist(hist)
    # prefer AUROC, then AUPRC/PR-AUC, then accuracy
    key, arr = _pick_first_metric(h, ['val_auc', 'val_auprc', 'val_pr_auc', 'val_accuracy'])
    if arr is not None:
        return int(np.nanargmax(arr))
    # fallback: last epoch of the first list in history
    any_list = next(iter(h.values()))
    return int(len(any_list) - 1)

def _stats_at(hist, idx):
    h = _ensure_hist(hist)
    def pick(key):
        arr = h.get(key, [])
        return float(arr[idx]) if (arr and 0 <= idx < len(arr)) else float("nan")
    train_stats = {
        "auc":       pick("auc"),
        "accuracy":  pick("accuracy"),
        "precision": pick("precision"),
        "recall":    pick("recall"),
        "loss":      pick("loss"),
    }
    # include common validation keys (AUPRC variants too)
    val_stats = {
        "auc":       pick("val_auc"),
        "auprc":     (pick("val_auprc") if not np.isnan(pick("val_auprc")) else pick("val_pr_auc")),
        "accuracy":  pick("val_accuracy"),
        "precision": pick("val_precision"),
        "recall":    pick("val_recall"),
        "loss":      pick("val_loss"),
    }
    return train_stats, val_stats

def _first_available(globs, names):
    for n in names:
        if n in globs:
            return globs[n]
    return None

# ---- collect histories with broader name coverage ----
G = globals()
hist_basic       = _first_available(G, ["history_basic", "history", "hist"])
hist_doubleconv  = _first_available(G, ["history_doubleConv", "history_doubleconv", "history_double"])
hist_doubledeep  = _first_available(G, ["history_doubleDeep", "history_doubledeep"])
hist_deep        = _first_available(G, ["history_deep", "history_resnet", "history_resnet18"])
hist_eff_warm    = _first_available(G, ["hist_eff_warm", "history_eff_warm"])
hist_eff_ft      = _first_available(G, ["hist_eff_ft", "history_eff_ft"])

names, hists = [], []
if hist_basic      is not None: names.append("Basic");        hists.append(hist_basic)
if hist_doubleconv is not None: names.append("Double Conv");  hists.append(hist_doubleconv)
if hist_doubledeep is not None: names.append("Double Deep");  hists.append(hist_doubledeep)
if hist_deep       is not None: names.append("Deep");         hists.append(hist_deep)
if hist_eff_warm   is not None: names.append("EffB0 Warm");   hists.append(hist_eff_warm)
if hist_eff_ft     is not None: names.append("EffB0 FT");     hists.append(hist_eff_ft)

if not hists:
    raise RuntimeError("No valid histories found. Train at least one model before running Conclusion.")

# ---- pick the best by preferred val metric ----
candidates = []
for name, h in zip(names, hists):
    idx = _best_epoch_idx(h)
    tr, va = _stats_at(h, idx)
    # scoring preference: AUROC -> AUPRC -> accuracy
    score = (va["auc"] if not np.isnan(va["auc"])
             else (va["auprc"] if (("auprc" in va) and not np.isnan(va["auprc"])) else va["accuracy"]))
    candidates.append((name, idx+1, tr, va, float(score)))

# best by score; tie-break by later epoch
best = sorted(candidates, key=lambda t: (t[4], t[1]))[-1]
best_name, best_epoch, train_stats, val_stats, _ = best

# ---- bar plot of val metrics (show AUROC, AUPRC, Acc, Prec, Rec if available) ----
keys = ["auc", "auprc", "accuracy", "precision", "recall"]
labels, vvals = [], []
for k in keys:
    if k in val_stats and not np.isnan(val_stats[k]):
        labels.append(k.upper() if k in ["auc","auprc"] else k.capitalize())
        vvals.append(val_stats[k])

plt.figure(figsize=(6,4))
plt.bar(labels, vvals)
for i, v in enumerate(vvals):
    plt.text(i, min(0.98, v + 0.02), f"{v:.3f}", ha="center", va="bottom")
plt.ylim(0, 1.0)
plt.title(f"Best Model: {best_name} (Epoch {best_epoch}) — Validation Metrics")
plt.ylabel("Score")
plt.grid(axis="y", alpha=0.3)
plt.tight_layout()
plt.show()

def _round_dict(d):
    out = {}
    for k, v in d.items():
        if isinstance(v, float) and np.isnan(v):
            out[k] = None
        else:
            out[k] = round(float(v), 4) if isinstance(v, (float, int)) else v
    return out

print(f"=== Best Model: {best_name} — Epoch {best_epoch} ===")
print("Train:", _round_dict(train_stats))
print("Val  :", _round_dict(val_stats))

"""### Step 17 - Validation ROC, PR, and Optimal Threshold (F1)

To evaluate how well the selected model separates the positive and negative classes, I computed several validation-set diagnostics:

**1. ROC Curve (AUROC).**  
The ROC curve shows strong separation between classes, with an **AUROC ≈ 0.929**. The curve sits far above the diagonal, indicating that the model ranks cancer vs. non-cancer patches effectively across a wide range of thresholds.

**2. Precision–Recall Curve (AUCPR).**  
Since this is an imbalanced dataset, the PR curve is often more informative than ROC.  
The **AUCPR ≈ 0.907**, demonstrating that the model maintains high precision even at high recall levels.

**3. Threshold Sweep for F1.**  
Rather than arbitrarily using a 0.50 cutoff, I swept thresholds between 0.05 and 0.95 and calculated the F1-score at each step.  
The best operating point occurs at approximately:

- **Best Threshold:** 0.252  
- **Best F1:** 0.831

This threshold achieves a better balance between precision and recall than the default 0.5.

**4. Confusion Matrices.**  
I printed confusion matrices at both `thr=0.5` and the optimized threshold `thr=0.252`.  
As expected, the optimized threshold yields a more favorable balance between false positives and false negatives, confirming that threshold tuning meaningfully improves detection performance on this dataset.

"""

# === Validation ROC, PR, AUCPR, optimal threshold (F1), and Confusion Matrices ===
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (
    roc_curve, auc,
    precision_recall_curve, average_precision_score,
    f1_score, confusion_matrix
)

# 1) Predict on validation
probs  = model.predict(val_gen, verbose=0).ravel()   # change `model` if your best model has a different name
y_true = val_gen.classes

# 2) ROC (AUROC)
fpr, tpr, _ = roc_curve(y_true, probs)
auroc = auc(fpr, tpr)
plt.figure(figsize=(5,4))
plt.plot(fpr, tpr, label=f"AUROC={auroc:.3f}")
plt.plot([0,1],[0,1],'k--')
plt.xlabel("FPR"); plt.ylabel("TPR"); plt.title("Validation ROC")
plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()

# 3) PR (AUCPR)
prec, rec, thr = precision_recall_curve(y_true, probs)
aucpr = average_precision_score(y_true, probs)
plt.figure(figsize=(5,4))
plt.plot(rec, prec, label=f"AUCPR={aucpr:.3f}")
plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title("Validation PR")
plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()

# 4) Threshold sweep (F1) to pick operating point
cand_thr = np.linspace(0.05, 0.95, 50)
f1_vals  = []
for t in cand_thr:
    y_hat = (probs >= t).astype(int)
    f1_vals.append(f1_score(y_true, y_hat, zero_division=0))
best_idx = int(np.argmax(f1_vals))
best_thr = float(cand_thr[best_idx])
best_f1  = float(f1_vals[best_idx])

plt.figure(figsize=(5,4))
plt.plot(cand_thr, f1_vals)
plt.axvline(best_thr, ls='--')
plt.xlabel("Threshold"); plt.ylabel("F1"); plt.title(f"F1 vs Threshold (best={best_f1:.3f} @ {best_thr:.3f})")
plt.grid(alpha=0.3); plt.tight_layout(); plt.show()

print(f"Selected operating threshold by F1: {best_thr:.3f} (F1={best_f1:.3f})")
print(f"AUROC={auroc:.4f} | AUCPR={aucpr:.4f}")

# 5) Confusion matrices @0.5 and @best_thr
for t in [0.5, best_thr]:
    y_hat = (probs >= t).astype(int)
    cm = confusion_matrix(y_true, y_hat)
    print(f"\nConfusion Matrix @ thr={t:.3f}:\n{cm}")

"""### Step 18 - Calibration — Reliability Curve and Brier Score

To understand how well the model’s predicted probabilities reflect true outcome frequencies, I evaluated **probability calibration** on the validation set.

I plotted a **reliability diagram**, where:
- The x-axis is the predicted probability (model confidence)
- The y-axis is the actual observed frequency of the positive class
- Perfect calibration lies on the diagonal (blue dashed line)

The curve shows that as predicted probability increases, the observed frequency also increases, meaning the model is reasonably calibrated. There is some under-confidence at mid-range probabilities, but the trend remains close to the ideal line.

I also computed the **Brier score**, a proper scoring rule for calibration and sharpness:
- **Brier score ≈ 0.115**

Since lower is better, this value indicates good calibration quality for a binary classification problem of this scale.

"""

# === Calibration: reliability diagram + Brier score (on validation) ===
import numpy as np, matplotlib.pyplot as plt
from sklearn.calibration import calibration_curve
from sklearn.metrics import brier_score_loss

# assumes you already have:
# probs = best_model.predict(val_gen, verbose=0).ravel()  OR model.predict(...)
# y_true = val_gen.classes
if 'probs' not in globals():
    probs = model.predict(val_gen, verbose=0).ravel()
y_true = val_gen.classes

brier = brier_score_loss(y_true, probs)
prob_true, prob_pred = calibration_curve(y_true, probs, n_bins=15, strategy='quantile')

plt.figure(figsize=(5,4))
plt.plot([0,1],[0,1],'--')
plt.plot(prob_pred, prob_true, marker='o')
plt.xlabel('Predicted probability'); plt.ylabel('Observed frequency')
plt.title(f'Reliability diagram (Brier={brier:.4f})')
plt.grid(alpha=0.3); plt.tight_layout(); plt.show()

print(f"Brier score = {brier:.4f}")

"""### Step 19 - Error Analysis — Inspecting Misclassified Samples

To better understand where the model struggles, I performed an **error analysis** by visualizing:

- **False Positives (FP):** predicted malignant but actually benign  
- **False Negatives (FN):** predicted benign but actually malignant

Using the chosen operating threshold (from the F1 optimization step), all FP and FN samples in the validation set were located and a small set of them (up to 36 per group) were plotted as image montages.

These montages provide qualitative insight into failure modes, such as:
- Very subtle lesions the model fails to detect (FN)
- Benign regions that visually resemble malignant tissue (FP)
- Low-contrast, noisy, or ambiguous patches

Reviewing these visual errors supports future improvements such as better augmentations, threshold tuning, or model refinement.

"""

# === Error analysis: visualize False Positives / False Negatives ===
import numpy as np, matplotlib.pyplot as plt, cv2, math

if 'probs' not in globals():
    mdl = globals().get('best_model', globals().get('model', None))
    if mdl is None:
        raise RuntimeError("No model found to compute validation predictions.")
    probs = mdl.predict(val_gen, verbose=0).ravel()

y_true = val_gen.classes
thr = float(best_thr) if 'best_thr' in globals() else 0.5
y_pred = (probs >= thr).astype(int)

fp_idx = np.where((y_true==0) & (y_pred==1))[0][:36]
fn_idx = np.where((y_true==1) & (y_pred==0))[0][:36]

def montage(idxs, title, cols=9):
    rows = max(1, math.ceil(len(idxs)/cols))
    plt.figure(figsize=(cols*1.1, rows*1.1))
    for k, i in enumerate(idxs):
        p = val_gen.filepaths[i]
        img = cv2.cvtColor(cv2.imread(p, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)
        plt.subplot(rows, cols, k+1)
        plt.imshow(img); plt.axis('off')
    plt.suptitle(title); plt.tight_layout(); plt.show()

montage(fp_idx, f'False Positives (thr={thr:.3f})')
montage(fn_idx, f'False Negatives (thr={thr:.3f})')

"""### Step 20 - Threshold Optimization — Selecting an Operating Point

After training the best-performing model, a **threshold sweep** was performed on the validation set to find the decision cutoff that maximizes **F1-score** (balancing precision and recall).

- Probabilities from the best model were evaluated at thresholds from **0.10 to 0.90**
- For each threshold, key metrics were computed: **Accuracy**, **Precision**, **Recall**, and **F1**

The resulting table clearly shows how model behavior changes as the threshold increases.  
From these results:

 **Recommended operating threshold (max F1): ~0.35**

This value offers the best balance between false positives and false negatives, and is therefore a strong candidate for deployment or further evaluation.

"""

# === Threshold sweep on the best model (optimize F1 or desired trade-off) ===
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

best_model = model_doubledeep  # pick your winner
probs = best_model.predict(val_gen, verbose=0).ravel()
y_true = val_gen.classes

ths = np.linspace(0.1, 0.9, 17)
rows = []
for t in ths:
    y_pred = (probs >= t).astype(int)
    rows.append([
        float(t),
        accuracy_score(y_true, y_pred),
        precision_score(y_true, y_pred, zero_division=0),
        recall_score(y_true, y_pred,   zero_division=0),
        f1_score(y_true, y_pred,       zero_division=0),
    ])

import pandas as pd
df_thr = pd.DataFrame(rows, columns=["Threshold","Acc","Prec","Rec","F1"]).round(4)
display(df_thr)

best_idx = int(df_thr["F1"].values.argmax())
best_thr = float(df_thr.iloc[best_idx]["Threshold"])
print(f"Recommended threshold (max F1): {best_thr:.3f}")

"""### Step 21 - Side-by-Side Confusion Matrices (0.50 threshold)

To compare how each architecture behaves on the validation set, we generated **confusion matrices** side-by-side for:

- **Basic CNN**
- **Double Conv**
- **Double Deep (BEST)**

Each model was evaluated at a fixed threshold of **0.50** and scored using:

-  **ROC-AUC** (preferred ranking metric)
-  **Accuracy**
-  Raw confusion-matrix cell counts
-  Row-normalized percentages

✔ The **best-ranked model** (based on highest AUROC, fallback Accuracy) is shown in **Blues**  
✔ Remaining models are shown in **Greens/Oranges** for clear visual contrast

This visualization highlights:
- True Positive vs False Negative performance
- Whether a model tends to over-predict or under-predict the positive class
- The relative gains achieved moving from Basic CNN → Double Conv → Double Deep

The **Double Deep** architecture achieved the best combination of AUC and Accuracy, and therefore is marked as **(BEST)** in the plot.

"""

# === Side-by-side Confusion Matrices: best in Blues, others in Greens/Oranges ===
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score

# Collect available models dynamically
models = []
if 'model' in globals():            models.append(("Basic CNN", model))
if 'model_double' in globals():     models.append(("Double Conv", model_double))
if 'model_doubledeep' in globals(): models.append(("Double Deep", model_doubledeep))

if len(models) == 0:
    raise RuntimeError("No models found in session (model/model_double/model_doubledeep).")

# Evaluate each model: compute AUC (fallback acc) and cache predictions
eval_rows = []
for name, mdl in models:
    probs  = mdl.predict(val_gen, verbose=0).ravel()
    y_true = val_gen.classes
    thr    = 0.50
    y_pred = (probs >= thr).astype(int)
    try:
        score_auc = roc_auc_score(y_true, probs)
    except Exception:
        score_auc = float('nan')
    score_acc = accuracy_score(y_true, y_pred)
    # choose ranking key (prefer AUC)
    rank_key = score_auc if np.isfinite(score_auc) else score_acc
    eval_rows.append({
        "name": name, "model": mdl, "probs": probs, "y_true": y_true, "y_pred": y_pred,
        "auc": score_auc, "acc": score_acc, "rank": rank_key
    })

# Identify the best model
best_idx = int(np.nanargmax([r["rank"] for r in eval_rows]))
best_name = eval_rows[best_idx]["name"]

# Colormap assignment: best=Blues; others cycle through Greens/Oranges
other_maps = ["Greens", "Oranges"]
cmaps = []
for i, row in enumerate(eval_rows):
    if i == best_idx:
        cmaps.append("Blues")
    else:
        cmaps.append(other_maps[(i - (1 if i > best_idx else 0)) % len(other_maps)])

# Plot
n = len(eval_rows)
fig, axes = plt.subplots(1, n, figsize=(5.2*n, 5.2))
if n == 1:
    axes = [axes]

for ax, row, cmap in zip(axes, eval_rows, cmaps):
    cm = confusion_matrix(row["y_true"], row["y_pred"], labels=[0, 1])
    with np.errstate(divide='ignore', invalid='ignore'):
        cm_row = cm / cm.sum(axis=1, keepdims=True)
        cm_row = np.nan_to_num(cm_row)

    im = ax.imshow(cm_row * 100, vmin=0, vmax=100, cmap=cmap)
    tag = " (BEST)" if row["name"] == best_name else ""
    auc_txt = f"{row['auc']:.3f}" if np.isfinite(row["auc"]) else "NA"
    ax.set_title(f"{row['name']}{tag}\nAUC={auc_txt} | Acc={row['acc']:.3f}")
    ax.set_xlabel("Predicted"); ax.set_ylabel("True")
    ax.set_xticks([0, 1]); ax.set_yticks([0, 1])
    ax.set_xticklabels(["0", "1"]); ax.set_yticklabels(["0", "1"])

    for i in range(2):
        for j in range(2):
            ax.text(
                j, i,
                f"{cm[i,j]}\n({cm_row[i,j]*100:.1f}%)",
                ha="center", va="center", fontsize=10,
                bbox=dict(boxstyle="round,pad=0.20", fc="white", ec="none", alpha=0.85)
            )

# One shared colorbar (Row %) across axes
cbar = fig.colorbar(im, ax=axes, fraction=0.02, pad=0.04)
cbar.set_label("Row %")

plt.tight_layout()
plt.show()

"""### Step 22 - Confusion Matrix for Best Model (Counts + Row-Percent)

The following visualization summarizes validation performance for the selected model  
(**Double Deep**, evaluated at threshold **0.50**):

-  **Rows = True labels**, Columns = **Predicted labels**  
-  Each cell shows **raw counts** and **row-normalized percentages**  
- Colormap: **Blues** (darker = higher proportion within each true class)

Reported metrics at this operating point:

- **Accuracy:** 0.941  
- **Precision:** 0.966  
- **Recall:** 0.886  
- **F1-score:** 0.924  
- **AUROC:** 0.983  

This chart makes class-wise behavior easy to interpret:
- The upper-left cell shows correct predictions for the *No-Cancer* class.
- The lower-right cell shows correct predictions for the *Cancer* class.
- The off-diagonal cells represent **false positives** and **false negatives**.

Overall, the model demonstrates high sensitivity and specificity, with low misclassification rates across both classes.

"""

# === Confusion Matrix (percent + counts) for a single model with Blue gradient ===
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# ----- choose the model you want to report -----
best_model = model_doubledeep   # e.g., model_doubledeep or model_double or model

# ----- predictions on validation set -----
probs  = best_model.predict(val_gen, verbose=0).ravel()
y_true = val_gen.classes
thr    = 0.50
y_pred = (probs >= thr).astype(int)

# ----- confusion matrix: counts & row-normalized percentages -----
cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
with np.errstate(divide='ignore', invalid='ignore'):
    cm_row = cm / cm.sum(axis=1, keepdims=True)
    cm_row = np.nan_to_num(cm_row)

# ----- metrics -----
acc = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred, zero_division=0)
rec  = recall_score(y_true, y_pred, zero_division=0)
f1   = f1_score(y_true, y_pred, zero_division=0)
try:
    auc_val = roc_auc_score(y_true, probs)
except Exception:
    auc_val = float('nan')

# ----- plot (Blues: light→dark) -----
fig, ax = plt.subplots(figsize=(5.4, 5.2))
im = ax.imshow(cm_row * 100, vmin=0, vmax=100, cmap="Blues")
ax.set_title(
    f"Confusion Matrix — thr={thr}\n"
    f"Acc={acc:.3f} | Prec={prec:.3f} | Rec={rec:.3f} | F1={f1:.3f} | AUC={auc_val:.3f}"
)
ax.set_xlabel("Predicted"); ax.set_ylabel("True")
ax.set_xticks([0, 1]); ax.set_yticks([0, 1])
ax.set_xticklabels(["0 (No Cancer)", "1 (Cancer)"], rotation=20)
ax.set_yticklabels(["0 (No Cancer)", "1 (Cancer)"])

# annotate: counts + percent
for i in range(2):
    for j in range(2):
        count = cm[i, j]
        pct   = 100.0 * cm_row[i, j]
        ax.text(
            j, i,
            f"{count}\n({pct:.1f}%)",
            ha="center", va="center", fontsize=11,
            bbox=dict(boxstyle="round,pad=0.25", fc="white", ec="none", alpha=0.85)
        )

cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
cbar.set_label("Row %")

plt.tight_layout()
plt.show()

"""### Step 23 -Model Comparison — Table and Metric-by-Metric Visualizations

In this step, I evaluated all of my trained models on the validation set and collected the key metrics: **Accuracy**, **AUC**, **Precision**, and **Recall**.  
I then created a comparison table and applied a separate color gradient to each column to make differences obvious:

- **Accuracy → Greens**
- **AUC → Blues**
- **Precision → Oranges**
- **Recall → Purples**

The table is automatically sorted by AUC (and then Accuracy), so the strongest model appears at the top.  
This makes it easy to visually identify which architecture performs best overall.

The first displayed image shows this comparison table with the color-coded columns. Models with higher scores in a metric column appear in stronger, darker colors, and lower values appear lighter. This visual style makes the ranking immediately interpretable.

The second image includes **four separate bar charts**, one figure per metric (AUC, Accuracy, Precision, Recall).  
Each chart uses a matching colormap and displays the exact numeric value above each bar.  
These plots allow me to see how each model behaves under each metric individually, instead of focusing on only one performance measure. The colorbar on each figure indicates how the shading corresponds to metric strength.

"""

# === Model Comparison: Table + Column-wise Color Gradients + Separated Metric Plots ===
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from matplotlib.cm import ScalarMappable
import matplotlib as mpl
from IPython.display import display

results = []

def _safe_eval(m, name):
    """Evaluate a model on val_gen and append metrics (Acc, AUC, Prec, Rec)."""
    try:
        vloss, vauc, vacc, vprec, vrec = m.evaluate(val_gen, verbose=0)
        results.append([name, float(vacc), float(vauc), float(vprec), float(vrec)])
    except Exception as e:
        print(f"[skip] {name}: {e}")

# Collect metrics from all available models (do not remove the table)
if 'model' in globals():            _safe_eval(model,            "Sequential CNN")
if 'model_double' in globals():     _safe_eval(model_double,     "Double Convolution")
if 'model_doubledeep' in globals(): _safe_eval(model_doubledeep, "Double Deep")

df_results = pd.DataFrame(results, columns=["Model", "Accuracy", "AUC", "Precision", "Recall"])
if df_results.empty:
    raise RuntimeError("No models evaluated. Train at least one model before running this cell.")

# Sort by AUC primarily, then Accuracy (stable mergesort keeps ties readable)
df_results = df_results.sort_values(by=["AUC", "Accuracy"], ascending=False, kind="mergesort").reset_index(drop=True)

print("=== Final Model Comparison (sorted by AUC, then Accuracy) ===")
display(df_results)

# Styled table with a distinct gradient per column (each column → its own range/cmap)
styler = (
    df_results.style
    .background_gradient(cmap="Greens",  subset=["Accuracy"])
    .background_gradient(cmap="Blues",   subset=["AUC"])
    .background_gradient(cmap="Oranges", subset=["Precision"])
    .background_gradient(cmap="Purples", subset=["Recall"])
    .format({"Accuracy": "{:.4f}", "AUC": "{:.4f}", "Precision": "{:.4f}", "Recall": "{:.4f}"})
)
display(styler)

# Identify best model by AUC (fallback already reflected by sorting)
best_row = df_results.iloc[0]
print(
    f"Best by AUC: {best_row['Model']}  |  "
    f"AUC={best_row['AUC']:.4f}, Acc={best_row['Accuracy']:.4f}, "
    f"Prec={best_row['Precision']:.4f}, Rec={best_row['Recall']:.4f}"
)

# ---------- Plot helper (each metric in its own figure with its own colormap) ----------
def _plot_metric_bar_gradient(df, metric, cmap_name, ylim=(0.0, 1.0)):
    """
    Bar chart with per-bar colors from a metric-specific colormap.
    Adds a colorbar and value annotations. Robust to MPL >=3.7.
    """
    vals  = df[metric].values.astype(float)
    names = df["Model"].values
    x     = np.arange(len(df))

    # Normalize to [0,1] for coloring
    norm = Normalize(vmin=0.0, vmax=1.0)
    cmap = mpl.colormaps.get_cmap(cmap_name)
    colors = cmap(norm(vals))

    fig, ax = plt.subplots(figsize=(7.2, 4.6))
    bars = ax.bar(x, vals, color=colors, edgecolor="black", linewidth=0.5)
    ax.set_xticks(x); ax.set_xticklabels(names, rotation=15, ha="right")
    ax.set_ylim(*ylim)
    ax.set_ylabel(metric)
    ax.set_title(f"Model Comparison — {metric}")
    ax.grid(axis="y", alpha=0.3)

    # Annotate values
    for i, v in enumerate(vals):
        ax.text(i, min(ylim[1]-0.02, v + 0.015), f"{v:.3f}",
                ha="center", va="bottom", fontsize=10)

    # Colorbar keyed to 0..1 (metric scale)
    sm = ScalarMappable(norm=norm, cmap=cmap)
    sm.set_array([])
    cbar = fig.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label(f"{metric} (0–1)")

    fig.tight_layout()
    plt.show()

# ---------- Individual metric plots (each column gets its own range/colormap) ----------
_plot_metric_bar_gradient(df_results, "AUC",       cmap_name="Blues")
_plot_metric_bar_gradient(df_results, "Accuracy",  cmap_name="Greens")
_plot_metric_bar_gradient(df_results, "Precision", cmap_name="Oranges")
_plot_metric_bar_gradient(df_results, "Recall",    cmap_name="Purples")

"""### Step 24 - Final Model Comparison on Validation :

In this step, I evaluated every trained model on the validation set using the **optimal threshold** that I previously selected from the F1 sweep.  
For each model, I computed:

- **AUROC**
- **AUCPR**
- **Precision**
- **Recall**
- **F1-score**
- **Total number of trainable parameters (in millions)**

All evaluated models are placed into a comparison table, which is then sorted primarily by **AUCPR** and secondarily by **AUROC**.  
This ranking is more informative than accuracy alone, especially on an imbalanced dataset, because AUCPR reveals how well the model separates the minority cancer-positive class.

The table clearly shows that:

- **`model_doubledeep`** achieves the strongest performance across nearly all metrics  
- **`model_double`** is the next best, with slightly lower AUCPR and F1
- The simple CNN (`model`) performs reasonably well but does not match the deeper architectures
- The pretrained EfficientNet baseline performs poorly on this dataset and threshold, confirming that transfer learning was not sufficient here

The final table below summarizes these results and makes the difference between architectures visually clear.

"""

# === Final model comparison on validation (@ chosen threshold) ===
import pandas as pd
from sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score, f1_score

thr = float(best_thr) if 'best_thr' in globals() else 0.5

def eval_on_val(name, mdl, gen, thr):
    p = mdl.predict(gen, verbose=0).ravel()
    y = gen.classes
    yhat = (p >= thr).astype(int)
    return {
        "Model": name,
        "AUROC": roc_auc_score(y, p),
        "AUCPR": average_precision_score(y, p),
        "Precision": precision_score(y, yhat, zero_division=0),
        "Recall": recall_score(y, yhat, zero_division=0),
        "F1": f1_score(y, yhat, zero_division=0),
        "Params(M)": mdl.count_params()/1e6
    }

rows = []
candidates = []
for n in ["model", "model_double", "model_doubledeep", "model_eff", "best_model"]:
    if n in globals():
        candidates.append((n, globals()[n]))

for n, m in candidates:
    rows.append(eval_on_val(n, m, val_gen, thr))

cmp_df = pd.DataFrame(rows).sort_values(["AUCPR","AUROC"], ascending=False)
display(cmp_df)

"""### Step 25 - Inference on `test/` and Creating `submission.csv`

In the final stage, I ran my chosen deployment model on all unseen test images and generated the `submission.csv` file required for Kaggle.

Steps performed in this cell:

1. **Select the best model automatically**  
   I let the code check all trained models (`best_model`, `model_eff`, `model_doubledeep`, etc.) and pick the first one that exists in the environment. This prevents errors if some models were not trained.

2. **Load all `.tif` images from the test folder**
   - Extract image IDs from filenames  
   - Resize and normalize each image using the same preprocessing pipeline used during training

3. **Apply Test-Time Augmentation (TTA)**
   - For each image batch, I compute predictions under multiple transformations  
     (horizontal flip, vertical flip, and 90° rotations)
   - I average the predictions to obtain a more stable score

4. **Apply the selected threshold**
   - I convert continuous probability outputs into binary predictions using the previously chosen optimal threshold

5. **Build and save `submission.csv`**
   - Two columns: `id` and `label`
   - Saved in the working directory, ready to upload to Kaggle

After this step finishes, the top rows of the file are printed and `submission.csv` is successfully created.

"""

# === Inference on test/ and build submission.csv ===
import numpy as np, pandas as pd, cv2
from glob import glob
from pathlib import Path
from tqdm import tqdm
import tensorflow as tf

# choose deployment model
deploy_model = None
for cand in ["best_model", "model_eff", "model_doubledeep", "model_double", "model"]:
    if cand in globals():
        deploy_model = globals()[cand]; break
if deploy_model is None:
    raise RuntimeError("No trained model found for inference.")

thr = float(best_thr) if 'best_thr' in globals() else 0.5
assert "TEST_DIR" in globals(), "TEST_DIR not defined."
assert "IMG_SHAPE" in globals(), "IMG_SHAPE not defined (e.g., (96,96,3))."
H, W = IMG_SHAPE[0], IMG_SHAPE[1]

test_files = sorted(glob(str(TEST_DIR / "*.tif")))
test_ids   = [Path(p).stem for p in test_files]
if len(test_ids)==0:
    raise RuntimeError("No test images found under TEST_DIR.")

USE_TTA = True
def tta_predict(model, x):
    preds = [model.predict(x, verbose=0)]
    if USE_TTA:
        preds += [
            model.predict(tf.image.flip_left_right(x), verbose=0),
            model.predict(tf.image.flip_up_down(x),  verbose=0),
            model.predict(tf.image.rot90(x, 1),      verbose=0),
            model.predict(tf.image.rot90(x, 2),      verbose=0),
            model.predict(tf.image.rot90(x, 3),      verbose=0),
        ]
    return np.mean(preds, axis=0).ravel()

def load_batch(paths):
    ims=[]
    for p in paths:
        im = cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)
        if im.shape[:2] != (H, W):
            im = cv2.resize(im, (W, H), interpolation=cv2.INTER_LINEAR)
        ims.append(im)
    x = np.stack(ims).astype('float32')/255.0
    return x

BS = 256
probs = np.zeros(len(test_ids), dtype=np.float32)
for s in tqdm(range(0, len(test_ids), BS)):
    paths = test_files[s:s+BS]
    xb = load_batch(paths)
    probs[s:s+len(xb)] = tta_predict(deploy_model, xb)

sub = pd.DataFrame({"id": test_ids, "label": (probs >= thr).astype(int)})
sub.to_csv("submission.csv", index=False)
print(sub.head(), "\nSaved -> submission.csv")

"""### Step 26 - Kaggle Submission

In this final optional step, I submit my `submission.csv` file directly to Kaggle from the notebook.  

To do this, I use the `kaggle.json` API token. The code:
- Copies `kaggle.json` into the correct directory,
- Sets the required permissions,
- Specifies the competition name (`histopathologic-cancer-detection`),
- Uploads `submission.csv` with a short submission message.

If the token is missing or the user does not have the Kaggle CLI installed, the cell gracefully skips without interrupting the notebook. This allows me to automatically push the predictions to Kaggle without leaving the notebook environment.

"""

# === Submit to Kaggle (optional; requires kaggle.json token) ===
import os
try:
    !mkdir -p /root/.kaggle
    if os.path.exists("/content/kaggle.json"):
        !cp /content/kaggle.json /root/.kaggle/
    elif os.path.exists("kaggle.json"):
        !cp kaggle.json /root/.kaggle/
    !chmod 600 /root/.kaggle/kaggle.json
    COMP='histopathologic-cancer-detection'
    !kaggle competitions submit -c $COMP -f submission.csv -m "best model + TTA (thr set from val F1)"
except Exception as e:
    print("Kaggle submit skipped or failed:", e)

"""### Step 27 - Saving Model Artifacts (Weights + Training Histories)

At the end of training and evaluation, I save everything needed for future inference or reproducibility:

1. **Best model weights**
   - The final deployment model is stored under `./models/best_model.h5`
   - If `.h5` export is not supported (e.g., some newer Keras models), the code saves in the TensorFlow SavedModel format instead

2. **Training histories**
   - Each history dictionary (loss, accuracy, AUC, etc. per epoch) is saved as a separate JSON file inside `./artifacts/`
   - Examples: `history_doubledeep.json`, `history_eff_ft.json`, etc.
   - These files allow me to reload results later for visualizations or reports without retraining

This ensures that the important outputs of the project are preserved and can be reused or shared.

"""

# === Save artifacts: model weights + histories ===
import os, json
os.makedirs("models", exist_ok=True)
os.makedirs("artifacts", exist_ok=True)

# save model
try:
    deploy_model.save("models/best_model.h5")
except:
    deploy_model.save("models/best_model")

# save histories if present
def save_hist(obj, fname):
    h = obj.history if hasattr(obj, "history") else (obj if isinstance(obj, dict) else None)
    if h is not None:
        with open(f"artifacts/{fname}", "w") as f:
            json.dump(h, f)

if 'hist' in globals():            save_hist(hist, "history_basic.json")
if 'hist_double' in globals():     save_hist(hist_double, "history_double.json")
if 'hist_doubledeep' in globals(): save_hist(hist_doubledeep, "history_doubledeep.json")
if 'hist_eff_warm' in globals():   save_hist(hist_eff_warm, "history_eff_warm.json")
if 'hist_eff_ft' in globals():     save_hist(hist_eff_ft, "history_eff_ft.json")

print("Saved: ./models/best_model*, ./artifacts/*.json")

"""##  Conclusion

In this mini-project, I implemented a complete deep learning pipeline for the **Histopathologic Cancer Detection** Kaggle competition. The task is a binary classification problem where each 96×96 RGB tile must be labeled as metastatic cancer (1) or non-cancer (0). The dataset includes over 220,000 training tiles and 57,000 unlabeled test tiles. Before modeling, I performed exploratory data analysis (EDA) to understand the dataset and inform later design decisions.

During EDA, I visualized random image samples, confirmed the RGB structure and resolution, inspected class balance, and verified that the images contain realistic histopathology textures rather than noise. The dataset is slightly imbalanced, so I computed **class weights** to counter this during training. Based on the visual and statistical observations, I planned to use convolutional neural networks with data augmentation, batch generation from disk, and validation monitoring via AUROC and AUCPR.

I trained several architectures:

-  **Baseline CNN**
-  **Double Convolution CNN**
-  **Double Deep CNN (best performer)**
-  **EfficientNetB0 pretrained on ImageNet** (fine-tuned)

All models were trained with:
- binary cross-entropy loss  
- precision, recall, F1, and AUROC metrics  
- early stopping  
- class weighting to correct imbalance  

After training, I compared all models using ROC curves, PR curves, AUCPR, confusion matrices, and a full metric comparison table. I also performed threshold optimization to maximize F1 instead of assuming 0.50. The **Double Deep CNN** achieved the strongest validation results with high AUROC, AUCPR, precision, recall, and F1, while keeping the parameter count moderate.

Next, I used the best model to run inference on the `test/` set with optional **test-time augmentation (TTA)** and created a **Kaggle-ready `submission.csv`** file. I saved all artifacts (model weights + histories) to ensure full reproducibility. Finally, I submitted the results to Kaggle and included a screenshot of my position on the leaderboard as requested by the project rubric.

Overall, the full pipeline covered:
- data loading and EDA  
- multiple models and tuning  
- evaluation with advanced metrics  
- threshold optimization  
- calibration and error analysis  
- Kaggle-ready inference and submission  
- saved artifacts for reproducibility

This demonstrates a complete applied CNN workflow for medical image classification.


---

##  Resources & References

- Kaggle Competition – *Histopathologic Cancer Detection*  
  https://www.kaggle.com/competitions/histopathologic-cancer-detection

- EfficientNet Paper & Documentation  
  - Tan & Le (2019), *EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks*  
  - TensorFlow Keras Applications: https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0

- Data Generators & Augmentation  
  - Keras `ImageDataGenerator` Documentation  
  - https://keras.io/api/preprocessing/image/

- ROC, Precision-Recall, Calibration & Metrics  
  - scikit-learn documentation: https://scikit-learn.org/stable/modules/model_evaluation.html  
  - Brier Score and calibration curve: sklearn.calibration

- Jupyter, Python, and Colab environment used for training

- Additional reference for GitHub notebook versioning:  
  https://www.reviewnb.com

All code, experiments, images, and results were produced by me within this notebook and linked GitHub repository.


---

**GitHub ** :
https://github.com/sepidfr/CNN-Cancer-Detection-Kaggle-Mini-Project
"""

